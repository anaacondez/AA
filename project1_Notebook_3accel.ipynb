{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1285000, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_data = pd.read_csv(r'C:\\Users\\lscon\\Desktop\\AA\\projeto\\the-three-body-problem\\mlNOVA\\X_train.csv')\n",
    "train_data = pd.read_csv(r'C:/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/3_body_problem/3_body_problem/X_train.csv')\n",
    "\n",
    "\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify faulty rows based on the criterion (all values = 0.0 except for Id)\n",
    "zero_rows = train_data[(train_data.drop('Id', axis=1) == 0).all(axis=1)]\n",
    "\n",
    "# Remove the faulty rows from the DataFrame\n",
    "train_data_preprocessed = train_data[~train_data.index.isin(zero_rows.index)]\n",
    "train_data_preprocessed.reset_index(drop=True, inplace=True)\n",
    "#train_data_preprocessed.to_csv('train_preprocessed.csv', index=False)\n",
    "\n",
    "np.linalg.matrix_rank(train_data_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the stats for the preprocessed data (without the rows with zeros)\n",
    "summary_stats_filtered = train_data_preprocessed.describe(include='all')\n",
    "\n",
    "#gives the stats for the nonprocessed data\n",
    "summary_stats = train_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix and plot it as a heatmap\n",
    "corr_matrix = train_data_preprocessed.drop(train_data_preprocessed.columns[13],\n",
    "                                       axis=1).corr()\n",
    "corr_matrix.to_excel('corr_matrix_train_processed.xlsx')\n",
    "\n",
    "plt.figure(figsize=(20, 16), dpi=800)\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.4f',\n",
    "            linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.savefig('Corr_matrix_heatmap.jpg', dpi=800)\n",
    "plt.show()\n",
    "\n",
    "# Create a pairwise scatter plot matrix\n",
    "scatter_matrix = pd.plotting.scatter_matrix(train_data_preprocessed,\n",
    "                                            figsize=(20, 20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairwise plots of correlation between variables\n",
    "\n",
    "rows = 50000\n",
    "x1_rows = x1.head(n=rows)\n",
    "partial_train_data = train_data_preprocessed.drop(columns=['v_x_1','v_x_2','v_y_1', \n",
    "                                                           'v_y_2', 'v_x_3', 'v_y_3']).head(n=rows)\n",
    "\n",
    "\n",
    "_= sns.pairplot(partial_train_data, kind=\"reg\", diag_kind=\"kde\", plot_kws={'line_kws':{'color':'red'}})\n",
    "plt.title('Pairwise plots t vs velocity components')\n",
    "plt.savefig('pairwisetv_50000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4997 [00:00<?, ?it/s]c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1783: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "100%|██████████| 4997/4997 [02:33<00:00, 32.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#create feature label matrices\n",
    "#we're not going to use the velocity components as features\n",
    "train_data_without_velocity = train_data_preprocessed.drop(columns=['Id','v_x_1','v_x_2','v_y_1', \n",
    "                                                           'v_y_2', 'v_x_3', 'v_y_3'])\n",
    "#divide by simulations\n",
    "list_of_times = [values for values in train_data_without_velocity['t']]\n",
    "time_index_tuples = list(enumerate(list_of_times))\n",
    "zeros_indexes = list(filter(lambda value: value[1] == 0, time_index_tuples))\n",
    "zeros_indexes = [value[0] for value in zeros_indexes] \n",
    "list_of_simulations = []\n",
    "lower_bound = 0\n",
    "for i in range(len(zeros_indexes)-1):\n",
    "    simulation = train_data_without_velocity.iloc[lower_bound:zeros_indexes[i+1]]\n",
    "    list_of_simulations.append(simulation)\n",
    "    lower_bound = zeros_indexes[i+1]\n",
    "from tqdm import tqdm\n",
    "\n",
    "#add label and put the starting position at every row\n",
    "#x1\n",
    "for simulation in tqdm(list_of_simulations):\n",
    "    first_row_values = simulation.head(1)\n",
    "    simulation.loc[:,'x_1_label'] = simulation.loc[:, 'x_1']\n",
    "    simulation.loc[:,'y_1_label'] = simulation.loc[:, 'y_1']\n",
    "    simulation.loc[:,'x_2_label'] = simulation.loc[:, 'x_2']\n",
    "    simulation.loc[:,'y_2_label'] = simulation.loc[:, 'y_2']\n",
    "    simulation.loc[:,'x_3_label'] = simulation.loc[:, 'x_3']\n",
    "    simulation.loc[:,'y_3_label'] = simulation.loc[:, 'y_3']\n",
    "    for index, row in simulation.iterrows():\n",
    "        simulation.at[index, 'x_1'] = first_row_values['x_1']\n",
    "        simulation.at[index, 'y_1'] = first_row_values['y_1']\n",
    "        simulation.at[index, 'x_2'] = first_row_values['x_2']\n",
    "        simulation.at[index, 'y_2'] = first_row_values['y_2']\n",
    "        simulation.at[index, 'x_3'] = first_row_values['x_3']\n",
    "        simulation.at[index, 'y_3'] = first_row_values['y_3']\n",
    "\n",
    "list_of_simulations_copy = list_of_simulations.copy()\n",
    "random.shuffle(list_of_simulations_copy) #we shuffle the data here so we are only shuffling different simulations and not amongst them\n",
    "all_simulations = pd.concat(list_of_simulations_copy, ignore_index=True)\n",
    "all_simulations.to_csv('feature_matrix.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the feature matrix file\n",
    "total_data = np.genfromtxt('feature_matrix.csv', delimiter=',')\n",
    "total_data = total_data[1:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training and validation\n",
    "sample, lixo = train_test_split(total_data, train_size=0.1, shuffle=False)\n",
    "data_train, data_temp = train_test_split(sample, train_size=0.5, shuffle=False)\n",
    "data_vali, data_test = train_test_split(data_temp, test_size=0.5, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the labels from the features\n",
    "features_train = data_train[:, :7]  \n",
    "labels_train = data_train[:, 7:] \n",
    "\n",
    "features_vali = data_vali[:, :7]  \n",
    "labels_vali = data_vali[:, 7:] \n",
    "\n",
    "features_test = data_test[:, :7]  \n",
    "labels_test = data_test[:, 7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accel_proxy(X):\n",
    "    c1 = X[:, [1,2]]\n",
    "    c2 = X[:, [3,4]]\n",
    "    c3 = X[:, [5,6]]\n",
    "\n",
    "    def _acc(c1, c2):\n",
    "        c = c1-c2\n",
    "        return c/(np.linalg.norm(c)**3)\n",
    "\n",
    "    a1 = _acc(c1, c2) + _acc(c1, c3)\n",
    "    a2 = _acc(c2, c1) + _acc(c2, c3)\n",
    "    a3 = _acc(c3, c1) + _acc(c3, c2)\n",
    "\n",
    "    X = np.hstack((X, a1, a2, a3))\n",
    "    return X\n",
    "\n",
    "accel_transformer = FunctionTransformer(func=accel_proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_test = accel_proxy(features_test)\n",
    "# features_vali = accel_proxy(features_vali)\n",
    "# features_train = accel_proxy(features_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainaccel = pd.DataFrame(data=features_train, columns=['t', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'a1x', 'a1y', 'a2x', 'a2y', 'a3x', 'a3y'])\n",
    "# trainaccel.to_csv('trainwithaccel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the points (isto e porque acho que era interessante termos no deck of slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entretanto descobri que o sklearn tem uma crossvalidation feature que e capaz de ser bem util no calculo do MSE (literalmente transformar\n",
    "# aquilo em duas linhas o que e fixolas)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#depois no final grafico para o melhor claro e calculamos sqrt(mse) para vermos o quao off estamos - test set\n",
    "#depois seria implementar isto tudo para as outras matrizes que faltam\n",
    "#sugeria depois transformar isto do modelo numa funcao para ser mais simples e nao repetirmos codigo\n",
    "# o mesmo poderia ser feito para a criaçao das matrizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Vali:\t1.1130009803004723\n",
      "RMSE Test:\t1.7359826264652185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('polynomial_features', PolynomialFeatures(7)),\n",
    "    ('regressor', Ridge(0.9))\n",
    "])\n",
    "\n",
    "pipeline.fit(features_train,labels_train)\n",
    "\n",
    "# Make predictions on the validation data using the best model\n",
    "labels_pred_vali = pipeline.predict(features_vali)\n",
    "\n",
    "# Evaluate the performance of the best model on the validation data\n",
    "mse_vali = mean_squared_error(labels_vali, labels_pred_vali, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "labels_pred_test = pipeline.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the best model on the test data\n",
    "mse_test = mean_squared_error(labels_test, labels_pred_test, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Feature Engineering - Model w/accelerations in the pipeline and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Train:\t1.2603472192997267\n",
      "RMSE Vali:\t4.314207075635748e+19\n",
      "RMSE Test:\t4.432581527546607e+19\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    ('acceleration_transformer', accel_transformer),\n",
    "    ('column_droper', ColumnTransformer([(\"stand_drop\", StandardScaler(), [0]+list(range(3,13)))])),\n",
    "    ('polynomial_features', PolynomialFeatures(5)),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline using features_train and labels_train\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "labels_pred_train = model.predict(features_train)\n",
    "\n",
    "mse_train = mean_squared_error(labels_train, labels_pred_train, squared=False)\n",
    "print(f\"RMSE Train:\\t{mse_train}\")\n",
    "\n",
    "# Make predictions on the validation data using the best model\n",
    "labels_pred_vali = model.predict(features_vali)\n",
    "\n",
    "# Evaluate the performance of the best model on the validation data\n",
    "mse_vali = mean_squared_error(labels_vali, labels_pred_vali, squared=False)\n",
    "print(f\"RMSE Vali:\\t{mse_vali}\")\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "labels_pred_test = model.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the best model on the test data\n",
    "mse_test = mean_squared_error(labels_test, labels_pred_test, squared=False)\n",
    "print(f\"RMSE Test:\\t{mse_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL WORLD DATA PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_realworld = pd.read_csv(r'C:/Users/duart/OneDrive/Ambiente_de_Trabalho/Master_Analysis_Engineering_Big_Data/23-24/1st_semester/AA_ML/Kaggle_challenges/3_body_problem/3_body_problem/X_test.csv')\n",
    "id_column = X_realworld['Id']\n",
    "print(id_column.head())\n",
    "X_realworld.drop('Id', axis=1, inplace=True)\n",
    "X_realworld.to_csv('X_realworld.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the preprocessed real-world data\n",
    "X_realworld = pd.read_csv('X_realworld.csv')\n",
    "predictions_realworld = model.predict(X_realworld)\n",
    "\n",
    "\n",
    "# Create a new Pandas DataFrame with the predictions\n",
    "df_predictions = pd.DataFrame(predictions_realworld)\n",
    "df_predictions.insert(loc=0, column='Id', value = id_column)\n",
    "df_predictions.columns=['Id', 'x_1', 'y_1', 'x_2', 'y_2', 'x_3', 'y_3']\n",
    "\n",
    "# Submit the Pandas DataFrame to the challenge creator\n",
    "df_predictions.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
